\documentclass[a4paper,english,numbers=noenddot,bibliography=totoc,chapterprefix=on,DIV=12]{scrartcl}

\usepackage[utf8x]{inputenc}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{amssymb,amsmath}
\usepackage{enumitem}
\PassOptionsToPackage{hyphens}{url}\usepackage{bookmark,hyperref}
\usepackage{multirow}
\usepackage[labelfont=bf,font=small]{caption}
\usepackage[font=footnotesize]{subcaption}
\usepackage{rotating}

\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{listings}

\setcapindent{0mm}

% boldmath in headings and toc, but not headers
\def\bfseries{\fontseries \bfdefault \selectfont \boldmath}

%allows footnotes in tabular
\usepackage{footnote}
\makesavenoteenv{tabular}
\makesavenoteenv{table}

% avoid space issues after macro
\usepackage{xspace}
\newcommand{\tof}{TOF\xspace}
\newcommand{\angstrom}{\textup{\AA}}

\newcommand{\Treduction}{t_{\text{reduction}}}
\newcommand{\Trun}{t_{\text{run}}}
\newcommand{\Tbin}{t_{\text{bin}}}
\newcommand{\Tevent}{t_{\text{event}}}
\newcommand{\Nbin}{N_{\text{bin}}}
\newcommand{\Ncore}{N_{\text{core}}}
\newcommand{\Ncoremean}{\langle N_{\text{core}}\rangle}
\newcommand{\Nevent}{N_{\text{event}}}
\newcommand{\Nreduction}{N_{\text{reduction}}}
\newcommand{\Nworkspace}{N_{\text{workspace}}}
\newcommand{\Nspec}{N_{\text{spec}}}
\newcommand{\Bmax}{B_{\text{max}}}
\newcommand{\Fevent}{f_{\text{event}}}
\newcommand{\Mcore}{M_{\text{core}}}

\newcommand{\beer}{BEER\xspace}
\newcommand{\bifrost}{BIFROST\xspace}
\newcommand{\cspec}{CSPEC\xspace}
\newcommand{\dream}{DREAM\xspace}
\newcommand{\estia}{ESTIA\xspace}
\newcommand{\loki}{LoKI\xspace}
\newcommand{\magic}{MAGIC\xspace}
\newcommand{\odin}{ODIN\xspace}

\newcommand{\mantid}{Mantid\xspace}
\newcommand{\nexus}{NeXus\xspace}

\makeatletter
\def\input@path{{generated/}}
\makeatother

\begin{document}

\title{Estimation of hardware requirements for data reduction with \mantid at ESS}
\author{Simon Heybrock\\
    {\small\href{mailto:simon.heybrock@esss.se}{\nolinkurl{simon.heybrock@esss.se}}}}

\maketitle

\tableofcontents

\section{Introduction}
\subsection{Overview}

The aim of this document is to capture our understanding of what hardware resources will be required for reducing data with \mantid for ESS.
While we attempt to give some estimates for, e.g., required core counts, it will become clear below there are too many uncertainties to specify hardware requirements.
In practice the main contribution of this document may thus be the description of how various parameters affect what hardware is required, allowing us to continuously update and improve our estimates.

Data reduction covers three main cases, live reduction, interactive reduction, and script-based reduction.

\paragraph{Live reduction}
Preliminary/simplified on-the-fly reduction of data and visualization is a key promise of ESS and will be required for all instruments, any time the beam is on, and at times also with beam off for other calibration work.
It is critical that this is available at all times so a dedicated pool of hardware may be required.

\paragraph{Interactive reduction} 
The general view is that data at ESS will be so large that users cannot do data reduction on their laptop or desktop PC. Therefore DMSC needs to provide resources for all interactive workflows using the \mantid GUI (MantidPlot or its successor) for data reduction and visualization.
This is required during a user visit as well as for a certain time period after the experiment.
Furthermore, we need to provide access to such resources \emph{before} an experiment such that users can familiarize themselves with the tools and provided software infrastructure, avoiding wasting beam time due to software problems.
During interactive reduction, CPU usages usually alternate between high and idle states, and the duration of each period is highly variable.
On the other hand, there is generally a large permanent memory requirement.
This means that clustering interactive sessions on a single compute node is only possible to a limited extent and will require nodes with a large amount of memory.

\paragraph{Script-based reduction}
Reduction of data based on a (Python) script.
This covers automatic reduction and batch reduction.
This is suitable for being run on a cluster using a standard queuing system with adequate means to ensure that queue lengths stay within limits.
For example, one could imagine that jobs for data reduction of a running experiment need to be prioritized over modelling jobs of a experiment that happened a long time ago.


\subsection{Threading and MPI support}
\label{sec:threading-and-mpi-support}

The \mantid framework is multithreaded and algorithms run from a script or interactively via the GUI will typically run with multiple threads.
To allow for scaling data reduction beyond a single node, MPI support has been added to \mantid.
MPI support is only available for script-based reduction.
In principle it could also be used for live reduction, provided that it is only a passive way to observe raw and reduced data.
There is no scope for MPI support connected to the GUI.\footnote{That is, with the current scope there will be no equivalent to the client-server MPI mode supported by, e.g., ParaView.}.

In various benchmarks we observed that generally an equivalent threaded run of a \mantid reduction script takes longer than the same reduction done with MPI:\footnote{This is true even without the optimized MPI-based NeXus loader.}
The scaling with the number of used cores is worse.
The reason for this is not fully understood but it is probably related to the fork-join threading approach.
Thus we cannot directly transfer results and scaling behaviors obtained for MPI to a threaded version of \mantid.
For machines with a moderate number of cores of up to roughly 10 cores we can probably use the rule of thumb that the threaded reduction will be $2\times$ slower than the MPI reduction.\footnote{Benchmark result that show this inferior scaling behavior can be found on the last 4 pages of \url{https://github.com/mantidproject/documents/blob/master/Performance/mpi-based_data_reduction_-_different_approach.pdf}}


\subsection{Ramp-up in hot commissioning and early operations}

The initial rates and pixel counts will be lower than the design values due to the ramp-up of accelerator power and incomplete detector coverage owing to build-out phases.
In Fig.~\ref{fig:accelerator-power} we show the estimate for the accelerator power ramp-up used in the following for scaling the expected event rates.
ESS instrument scientists expect a linear scaling of event rate in the useable wavelength band with the accelerator power.
In Tab.~\ref{tab:pixel-counts} we list the pixel counts for the first instruments, i.e., effectively $\Nspec$.
We do not list the imaging beamline \odin.
Given that the data format for this beamline is typically a stack of images it is not suitable for a joint estimation with the other beamlines, where data is a list of events or a spectrum for every pixel.
\odin's reduction hardware requirements should probably be considered jointly with its requirements for reconstruction and analysis.
To obtain $\Nbin$ we need to combine $\Nspec$ with the number of bins per spectrum.
This depends on the resolution of the instrument, which can be configurable.
Details for this and other parameters are listed in Sec.~\ref{sec:script-based-results}.

\begin{figure}
  \centering
\includegraphics[height=0.33\textheight]{accelerator-power.pdf}
\caption{\label{fig:accelerator-power}Estimate of accelerator power ramp up.}
\end{figure}

\begin{table}
  \centering
  \begin{tabular}{lrrl}
    Instrument & Pixel count day 1 & Pixel count final & Comment\\
    \hline
    \beer & 200k & 400k & not including SANS upgrade\\
    \bifrost & ? & 5k & low pixel count but scanning\\
    \cspec & 400k & 750k \\
    % 1.25*32*6.2/(0.003*0.006) = 14000k
    \dream & 4000 & 12000k & estimated\footnote{We did not yet receive pixel counts from DREAM, but MAGIC uses same technology with a pixel size of $3~\mathrm{mm} \times 6~\mathrm{mm}$ with 32 voxel layers. Full coverage is $5.11~\mathrm{sr}$ with a sample-detector distance of $1.25~\mathrm{m}$. Day-1 coverage is $1.81~\mathrm{sr}$.}\\
    \estia & 250k & 500k \\
    \loki & 750k & 1500k \\
    \magic & 1440k & 2880k \\
  \end{tabular}
  \caption{\label{tab:pixel-counts}Pixel counts for the first instruments. Generally instruments to not have full detector coverage at day 1 and will add more pixels in an upgrade, typically a couple of years later.}
\end{table}




\section{Live reduction}

The discussion in this section is based on live reduction benchmarks conducted by Lamar Moore.\footnote{Tested with an Intel Core i7 with 4 cores, 8 MByte cache, and 64 GByte RAM..}
See the reference given in App.~\ref{app:refs} for details.
In Tab.~\ref{tab:consumption-rate} we show benchmark results for the event consumption rate of the Kafka event listener in \mantid without processing or visualization.
This test is done using a default (non-MPI) build of \mantid with a version roughly equivalent to release 3.13.
MPI support for the Kafka listener is not implemented currently.

With the exception of the SANS2D data point at $10^5$ the scaling with the event rate is linear.\footnote{It is likely that for the high rate of more than 1000 frames per second other effects become relevant. Since we are not interested in such high frame rates we ignore this data point and assume linear scaling.}
The benchmarks also show a (very approximately) linear scaling of the consumption rate with the number of pixels.
This is not entirely expected.
There are two aspects that can be used to explain the scaling:
With more pixels the write behavior to the \verb|EventWorkspace| becomes more random leading to a less and less efficient use of cache and the rest of the memory subsystem.\footnote{We expect that this effect levels off once we approach 100\% cache miss rate, but it is unclear at what pixel count that will happen.}
In addition, a separate vector needs to be allocated for every pixel to store the events.
This allocation cost could grow linearly with the pixel count.

\begin{table}
  \centering
  \begin{tabular}{c|rrr}
    Instrument & SANS2D & MERLIN & WISH\\
    Pixel count & 122888 & 286729 & 778245\\
    \hline
    $10^5$ events in 14 frames & 1379 & 915 & 400 \\
    $10^6$ events in 14 frames & 284 & 93 & 38 \\
    $10^7$ events in 14 frames & 23 & 8 & 3 \\
  \end{tabular}
  \caption{\label{tab:consumption-rate}Number of consumed frames per second. For ESS there are 14 frames (pulses) per second, i.e., we require a consumption rate of at least 14. The update timeout is set to $1~\mathrm{s}$.}
\end{table}

By increasing the update timeout, which was set to one second in Tab.~\ref{tab:consumption-rate}, the consumption rate can be improved moderately.
A couple of other optimizations providing a small speedup, such as multithreading in the listener appear to be possible.
On the other hand, live display of the raw data using the \verb|InstrumentView|, (simplified) data reduction, and live display of the reduced data add extra overhead.
Combining these opposite effects, which --- according to preliminary benchmarks --- all appear to be in the order of a $2\times$ change, leads us to conclude that the numbers given in Tab.~\ref{tab:consumption-rate} are a reasonable estimate.
That is, without having access to much more detailed workflows and instrument models we have the crude estimate

\begin{align}
  R_{\text{live}} &= \frac{N_{\text{frame}}^{\text{max}}}{14} = \frac{a}{14\Fevent\Nspec} = \frac{2.7(1)\cdot10^{13}~\mathrm{s}}{14\Fevent\Nspec} = \frac{2\cdot10^{12}~\mathrm{s}}{\Fevent\Nspec},
\end{align}
where $N_{\text{frame}}^{\text{max}}$ is the maximum number of consumed frames as in Tab.~\ref{tab:consumption-rate}, 14 is the number of frames per second at ESS, and $\Fevent$ is the event rate.
The constant $a$ is obtained from a fit to the data in Tab.~\ref{tab:consumption-rate}, excluding the SANS2D data point at $10^5$.
We require $R_{\text{live}} > 1$ to be able to support live reduction without MPI.
In Fig.~\ref{fig:live-reduction-no-mpi} we show the implications for the first ESS instruments, depending on the accelerator power.
With the exception of \dream and possibly \magic, we seem to be able to handle most cases for most instruments even at higher accelerator power.
However, we will definitely require MPI support in live reduction for \dream from day one.

\begin{figure}
  \centering
\includegraphics[height=0.8317\textheight]{live-reduction-no-mpi.pdf}
\caption{\label{fig:live-reduction-no-mpi}Reduction speed ratio $R_{\text{live}}$ for the first ESS instruments.
With increasing accelerator power, $\Fevent$ increases and $R_{\text{live}}$ decreases.
We require $R_{\text{live}} > 1$ to be able to handle the data.
The plot range has been chosen accordingly --- a graph hitting the lower horizontal axis implies that the rate cannot be handled anymore.
We plot several different instrument configurations which differ in their event rate.
The configuration names for \loki refer to the collimation length, which influences the event rate.
For \bifrost the pixel count is very low and we are not confident that we can extrapolate the scaling benchmarks from Tab.~\ref{tab:consumption-rate} this far.
We have therefore excluded the panel for \bifrost but expect that the rate can be handled even at a power of $5~\mathrm{MW}$.
The imaging beamline \odin is a special case and has thus also been excluded from this figure.
}
\end{figure}

For experiments with very high throughput, i.e., quick changes of sample-environment parameters or sample, we have to deal with additional complexity in the live reduction workflow.
This could include more advanced filtering, display and comparison of multiple results at the same time,\footnote{e.g., waterfall plots} and overhead from frequent transitions to a new run.
We have not yet started development of these features so benchmarks are not available.
That being said, high throughput experiments are not likely to be required in the early days.
Furthermore, UX is likely to be a bigger issue than performance in this case.
We consider it unlikely that this scenario will cause a significant overall difference in the required hardware.




\section{Interactive reduction}

As discussed in Sec.~\ref{sec:threading-and-mpi-support} there is no provisioning for MPI support for the GUI or interactive reduction in general, limiting scaling with the number of available cores.
Generally we do not expect good scaling far beyond 10 cores.
For reductions that are too slow in that case there are several options that can be used in practice:
\begin{itemize}
  \item Use slow interactive reduction for a single run/sample, apply resulting workflow to other samples using script-based reduction on the cluster.
  \item Use interactive reduction using a reduced or compressed set of data, e.g., by loading only every $N$-th pulse or by compressing events, apply resulting workflow to other samples using script-based reduction on the cluster.
  \item Use slow interactive reduction and use different sessions to do something else in the meantime, such as reducing another sample.
\end{itemize}
With exception of the second option, there is a minimum amount of required total RAM.
We can attempt to estimate the requirement as

\begin{align}
  \label{eq:ram-interactive}
  M_{\text{interactive}} &= 2~\mathrm{GByte} + \Nspec \cdot 256~\mathrm{Byte} + N_{\text{workspace}}(24\Nbin + 16\Nevent)~\mathrm{Byte},
\end{align}
with
\begin{itemize}
  \item $\Nworkspace$ is the number of workspaces present in \mantid at the same time.
    For interactive reduction this can typically be larger than the absolute minimum required for a fixed script-based reduction.
    Therefore we use $\Nworkspace = 10$ here, which is higher than what we will require below for script-based reduction in Sec.~\ref{sec:script-based-results}.
  \item $2~\mathrm{GByte}$ base memory.
    We assume a larger base requirement than in Sec.~\ref{sec:script-based-results} for GUI requirements.
  \item $256~\mathrm{Byte}$ of memory per pixel to represent the instrument geometry.
    This is assumed to be shared between workspaces.
  \item $24~\mathrm{Byte}$ per bin per workspace for double-precision X, Y, and E.
  \item $16~\mathrm{Byte}$ per event per workspace for double-precision time-of-flight and pulse-time.
\end{itemize}
Considering the projected duration of a single run $\Trun$ for design-power event rates, most experiments seem to use roughly $10^8$ to $10^9$ events.
Based on this equation, we give example requirements for various instrument configurations in Fig.~\ref{fig:interactive-ram}.
Notable exceptions to this are the inelastic beamlines and the single-crystal beamlines, if we consider merging data from all sample rotation angles.
Traditionally single crystal diffraction and inelastic scattering have the highest memory requirements but it is currently unclear whether this merge step would be done in \mantid.

\begin{figure}
  \centering
\includegraphics[height=0.33\textheight]{interactive-ram.pdf}
\caption{\label{fig:interactive-ram}
Scaling of memory requirements for interactive reduction with the event count per run.
The plot uses the final pixel count from Tab.~\ref{tab:pixel-counts} and $\Nbin$ based on Tab.~\ref{tab:resolution}.
The powder and single crystal diffractometers \dream, \magic, and \beer show an exceedingly high memory requirement due to the high pixelation and time resolution.
This is probably not a real and realistic requirement, see the discussion in Sec.~\ref{sec:script-based-results}.
The worst-case assumption that a histogram-representation for all workspaces is required leads to a rather high baseline requirement --- scaling with the total event count only becomes visible near $10^9$ events and above.
Excluding the special cases listed above, the required memory will mostly not exceed $256~\mathrm{GByte}$ or up to $1~\mathrm{TByte}$ for large runs with $10^{10}$ events.
The requirement can be lowered further by reducing $\Nworkspace$ and/or by avoiding binning of most workspaces, i.e., working purely in event mode.
}
\end{figure}

Instrument scientists expect that typically 5 interactive sessions would be required per instrument at any given time.
This figure is to be combined with requirements for data analysis since typically the interactive sessions would be used for both, data reduction and data analysis.
For data reduction, a lot of the time \mantid will actually be idle or unused.
Thus simply adding the required 5 sessions to the required numbers for data analysis is likely to overestimate the required resources.




\section{Script-based reduction}


\subsection{Scaling analysis}
\label{sec:scaling-analysis}

The number of cores required for MPI-based reduction depends on the required maximum runtime for a reduction.
It is unclear what a good limit for the runtime $\Treduction$ is.
For now we define:

\begin{itemize}
  \item The reduction should be $5\times$ faster than the experiment.
  \item To ensure that long-running experiments with low event rates are processed quickly we require that a minimum of $2\cdot10^6$ events/s are processed.
  \item We never require $\Treduction < 30$ seconds.
\end{itemize}
A visualization of these requirements is given in Fig.~\ref{fig:reduction-time-requirement}.

\begin{figure}
  \centering
\includegraphics[height=0.33\textheight]{reduction-time-requirement.pdf}
\caption{\label{fig:reduction-time-requirement}Imposed requirements for the maximum reduction time. The checked areas indicate regions that are forbidden by the defined limits. For runs that are short relative to the event rate we are governed by the 30-second limit. Above that limit, at low event rates the required reduction duration is low compared to the run duration. With increasing event rate we get closer to the limit of $5\times$ speedup indicated by the bold red line.}
\end{figure}

With a couple of approximations (which are probably minor for this purpose and compared to other sources of uncertainty) we can describe the time required to reduce a set of data as

\begin{align}
  \Treduction = t_0 + \frac{\Nbin\Tbin + \Nevent\Tevent}{\Ncore} + \frac{\Nevent}{\Bmax},
  \label{eq:master}
\end{align}
where
\begin{itemize}
  \item $\Nbin$ is the number of bins in the workflow, i.e., the number of spectra $\Nspec$ multiplied by the number of bins per spectrum.\footnote{Typically $\Nspec$ is the number of pixels of the instrument, but it can be different, e.g., when multiple adjacent pixels are grouped due to a lower resolution requirement or due to potential projections related to volumetric detectors.}
    The number of bins per spectrum depends on the bandwidth and resolution of the instrument.
    As a rule of thumb, for a given energy resolution $\delta E$ we require a bin size of $\delta E/10$.
  \item $\Nevent$ is the total number of events that are being handled in the reduction workflow.
    This can include events from multiple files, e.g., for a sample run and a background run.
  \item $\Ncore$ is the number of cores (MPI ranks) used in the reduction.
    We are not considering a hybrid threading+MPI approach.\footnote{A few specialized algorithms are using threading internally, even in an MPI run.}.
  \item $t_0$ is a constant time specific to the reduction workflow.
    It includes anything that does not depend and the number of spectra or number of events.
    Typically this includes small parts of the time spend in every algorithm, time for loading experiment logs from NeXus files, time for loading auxiliary files, and other overheads.
  \item $\Tbin$ is the (computed) time to run the workflow for a single bin.
  \item $\Tevent$ is the (computed) time to run the workflow for a single event.
  \item $\Bmax$ is the number of events that can be loaded from the file system per second.
\end{itemize}
The equation is motivated by an analysis of benchmark results --- see Sec.~\ref{sec:benchmarks} --- and the computational structure of data reduction.
$\Ncore$ can be adjusted such that $\Treduction$ fulfills the conditions listed above.
There can be cases where the conditions are violated, e.g., when the event rate is high and $\Bmax$ is too low.
More details on this equation can be found in App.~\ref{app:master_eq}.

Our benchmarks based on existing instruments and investigations of parameters for a series of ESS instruments show that any of the terms in Eq.~\eqref{eq:master} can be relevant or even dominant, depending on the instrument, the experiment, and the number of used cores:

\begin{itemize}
  \item 
    For instruments that produce many small files with few spectra and events, the $t_0$ term can become dominant.
    This can in theory be improved by processing multiple files in the same workflow, but at this point making such an assumption is not justified.
    It is thus important to capture the typical run duration for each instrument.
  \item
    Even if the $t_0$ term is not dominant, a run may contain relatively few events relative to the pixel count.
    In that case the $\Nbin$ term is important.
    Given that many ESS instruments have a high pixel count this could very well become a dominating factor.\footnote{In many experiments the total number of events required for each measurement may not increase drastically over present-day experiments. However, the ESS event \emph{rate} and pixel counts are often much higher, so the balance may shift to our disadvantage.}
  \item
    For instruments with many events we may be using many cores to offset the reduction cost.
    In that limit the bandwidth-limiting term becomes relevant.
\end{itemize}

Based on the time for reducing a single run we can thus compute the average number of cores required for reducing data for the instrument.
Note that this does \emph{not} include live reduction and interactive sessions.
We define

\begin{align}
  \Ncoremean = p_{\text{use}}\Nreduction \Ncore \frac{\Treduction}{\Trun},
\end{align}
where

\begin{itemize}
  \item $p_{\text{use}}$ is the probability that the instrument is in use in a specific operating mode.
  Typically there are multiple operating modes for an instrument so there will be multiple $\Ncoremean$ values that need to be be summed.
  \item $\Nreduction$ is the number of times data is reduced.
    Typically this should be small, e.g., 1, 2, or 3, but especially in the early days there will be exceptions.
  \item $\Trun$ is the duration of a single run.
\end{itemize}
For convenience, we can expand the master equation and obtain

\begin{align}
  \Ncoremean &= p_{\text{use}}\frac{\Nreduction}{\Trun}\left[\Ncore \left(t_0 + \frac{\Nevent}{\Bmax}\right) + \Nbin\Tbin + \Nevent\Tevent\right]\\
  \label{eq:master-v2}
   &= p_{\text{use}}\Nreduction\left[\Ncore \left(\frac{t_0}{\Trun} + \frac{\Fevent}{\Bmax}\right) + \frac{\Nbin\Tbin}{\Trun} + \Fevent\Tevent\right]\\
   &= p_{\text{use}}\Nreduction\left[\Ncore \left(\frac{t_0\Fevent}{\Nevent} + \frac{\Fevent}{\Bmax}\right) + \frac{\Nbin\Tbin\Fevent}{\Nevent} + \Fevent\Tevent\right].
\end{align}
All three lines are trivial variations of each other and are merely listed to highlight different aspects of the scaling behavior.

It is important to note that $\Ncoremean$ depends on $\Ncore$, i.e., the more cores we use, the higher our overall hardware requirement.
Reasons for using more cores are primarily (1) to reduce the time for a single reduction to something that is acceptable for users, and (2) to work around memory limitations on a single node.
There are two terms that depend on $1/\Trun$ (see Eq.~\eqref{eq:master-v2}), i.e., shorter runs will increase the number of required cores even if all other parameters such as the event rate $\Fevent$ are unchanged.

%Under the assumption that we are not bound by any of the other limits we can also substitute the $5\times$ speedup factor and eliminate $\Ncore$.
%Then Eq.~\eqref{eq:master} yields
%
%\begin{align}
%  \frac{\Trun}{5} &= t_0 + \frac{\Nbin\Tbin + \Nevent\Tevent}{\Ncore} + \frac{\Nevent}{\Bmax},\\
%  \Rightarrow \Ncore &= \frac{\Nbin\Tbin + \Nevent\Tevent}{\frac{\Trun}{5} - t_0 - \frac{\Nevent}{\Bmax}},\\
%  \Rightarrow \Ncoremean &= \Nreduction\left[ \frac{\Nbin\Tbin + \Nevent\Tevent}{\frac{\Trun}{5} - t_0 - \frac{\Nevent}{\Bmax}} \left(\frac{t_0}{\Trun} + \frac{\Fevent}{\Bmax}\right) + \frac{\Nbin\Tbin}{\Trun} + \Fevent\Tevent\right].
%\end{align}


\subsection{Event filtering}

The discussion in this section is based on event filtering benchmarks conducted by Neil Vaytet.
See the reference given in App.~\ref{app:refs} for details.
Many ESS instruments will require filtering events in one way or another.
Benchmarks indicate that the contribution to the overall runtime is less than $10\%$, i.e., we do not need to handle it explicitly in our evaluations.
While the cost of filtering itself is not significant, in many cases we are dealing with many more workspaces after the filtering process, e.g., one for every temperature interval in a temperature scan.
If the majority of data reduction can be done \emph{before} filtering this is not a problem.
However, if a significant portion of data treatment can only happen after the filtering/splitting process the cost is increasing significantly since we are effectively dealing with many distinct runs.
This problem is probably similar to the case of sample rotation angles discussed later in Tab.~\ref{tab:hardware-magic} for \magic and needs to be investigated closer in the future.


\subsection{Baseline benchmarks}
\label{sec:benchmarks}

To establish an estimate for the parameters $t_0$, $\Tbin$, $\Tevent$, and $\Bmax$ in Eq.~\eqref{eq:master} a number of benchmarks have been undertaken.\footnote{Most MPI-related changes have been included in the \emph{master} branch as of release 3.12 and 3.13.}

\begin{enumerate}
  \item Using an MPI-build of \mantid we measured run times of workflows for SANS, powder diffraction, and direct geometry spectroscopy (using the ISIS workflow for SANS2D, the \verb|SNSPowderDiffraction| workflow for PG3, and the \verb|DgsReduction| workflow for CNCS).
    We studied the scaling behavior with the number of used cores (MPI ranks), the number of events, and also the number of bins per spectrum.
    By looking at the various limits (few/many cores and few/many events) we can extract an approximation for $t_0$, $\Tbin$, $\Tevent$.
    All these experiments were done on a single node, a dual-socket Xeon E5-2620 v3 running at 2.40GHz.
  \item Scaling beyond a single node has been established in older benchmarks in 2015, again using the \verb|SNSPowderDiffraction|.
    This benchmark showed that scaling beyond a single node does not lead to any significant slowdown and scaling works well up to a high number of MPI ranks, with the exception of algorithms loading or saving files.
  \item For the special case of I/O we rely on benchmarks for the recently optimized parallel event loader for \nexus files.
    Currently only benchmarks with a local SSD are available and show scaling up to nearly $10^8$ loaded events per second (slightly lower for compressed files) with 10 used cores.
    Benchmarks with a parallel file system are pending.
\end{enumerate}
The experiments show a surprisingly consistent pattern, independent of the technique:

\begin{itemize}
\item $t_0$ is about 10 seconds.
\item $\Tbin$ varies from 1/1.000.000 seconds for histogram-heavy reductions to about 1/10.000.000 seconds for reductions with near-ubiquitous use of event-mode.
\item $\Tevent$ is about 1/1.000.000 seconds, or slightly smaller.
\item $\Bmax$ is about 50.000.000 events/second for an SSD, tests with a parallel file system are pending.
\end{itemize}


\subsection{Other factors}

With low accelerator power and lower detector coverage in the early days, it is to be expected that the type of experiments will be different from what is envisioned for full power.
For example, larger samples will be used to compensate for the lower rates.
The compensation could be of up to an order of magnitude in size, at least for certain experiments.
While some instruments indicate they will do so, e.g., \loki and \dream, not all of them will, e.g., \cspec.
We have \emph{not} included a scaling term to represent this effect in the numbers given in this document.
Ensuring that the compute cluster size ramp up is sufficiently ahead of the accelerator power ramp up could be the solution to dealing with this uncertainty.


\subsection{Results}
\label{sec:script-based-results}

This section discusses results obtained for script-based reduction with the performance model introduced earlier.
In the following we give example listings of reduction times, core counts, and memory consumption for various instruments and operating modes.
We strongly emphasize that the resulting values heavily depend on parameters that we cannot estimate with good accuracy.
Parameters used for generating the listings can be found in Tab.~\ref{tab:hardware-estimation-parameters}.
Resolution options and the resulting bin counts are given in Tab.~\ref{tab:resolution}.
Python scripts to generate the tables are described in App.~\ref{app:scripts}.
The resulting data for the given parameters can be found in Tabs.~\ref{tab:hardware-beer}--\ref{tab:hardware-magic}.

\begin{table}
  \centering
  \begin{tabular}{lrp{8cm}}
    Parameter & Value & Comment\\
    \hline
    $t_0$ & $10~\mathrm{s}$ \\
    $\Tbin$ & $2\times10^{-7}~\mathrm{s}$ \\
    $\Tevent$ & $10^{-6}~\mathrm{s}$ \\
    $\Bmax$ & $10^8~\mathrm{s}^{-1}$ \\
    $\Nworkspace$ & 5 \\
    $\Nreduction$ & 2 \\
    $\Ncore^{\text{max}}$ & 512 & set as arbitrary limit to reduce the number of nonsensical results from special parameter combinations in conflict with generic performance requirements from Sec.~\ref{sec:scaling-analysis} \\
    \hline
  \end{tabular}
  \caption{\label{tab:hardware-estimation-parameters}
  List of parameters used to generate Tabs.~\ref{tab:hardware-beer}--\ref{tab:hardware-magic}.
  See also Eq.~\eqref{eq:ram} regarding memory requirements and related parameters.
}
\end{table}

\begin{table}
  \centering
  \begin{tabular}{llrl}
    Instrument & Mode & Bins per spectrum & Comment \\
    \hline
    \hline
    \multirow{2}{*}{\beer}
    & medium-flux & 8500 \\
    & high-flux & 2000 \\
    \hline
    \multirow{1}{*}{\bifrost}
    & average & 850\\
    & high-flux & 110 & is highest flux always lowest resolution?\\
    \hline
    \multirow{1}{*}{\cspec}
    & (all) & 1000 & no large resolution variation possible? \\
    \hline
    \multirow{3}{*}{\dream}
    & high-resolution & 71000 \\
    & medium & 10000 \\
    & high-flux & 1420 \\
    \hline
    \multirow{1}{*}{\estia}
    & (all) & 450 \\
    \hline
    \multirow{1}{*}{\loki}
    & (all)    & 240 \\
    \hline
    \multirow{1}{*}{\magic}
    & (all) & 7100 \\
    \hline
  \end{tabular}
  \caption{\label{tab:resolution}
List of bin counts used to generate Tabs.~\ref{tab:hardware-beer}--\ref{tab:hardware-magic}.
Bin counts have been estimated based on instrument proposals and input from instrument scientists.
Details can be found in App.~\ref{app:instrument-parameters}.
}
\end{table}

We adopt a simplified model to estimate the amount of required main memory.\footnote{This is similar to but different from Eq.~\eqref{eq:ram-interactive} used for the case of interactive reduction.}
We define

\begin{align}
  \label{eq:ram}
  M &\equiv \Ncore(1~\mathrm{GByte} + \Nspec \cdot 256~\mathrm{Byte}) + N_{\text{workspace}}(24\Nbin + 16\Nevent)~\mathrm{Byte},\\
  \Mcore &\equiv \frac{M}{\Ncore} = 1~\mathrm{GByte} + \Nspec \cdot 256~\mathrm{Byte} + \frac{\Nworkspace}{\Ncore}(24\Nbin + 16\Nevent)~\mathrm{Byte}.
\end{align}
with
\begin{itemize}
  \item $1~\mathrm{GByte}$ base memory per core.
  \item $256~\mathrm{Byte}$ of memory per pixel per core to represent the instrument geometry.
    This is assumed to be shared between workspaces.
  \item $24~\mathrm{Byte}$ per bin per workspace for double-precision X, Y, and E.
  \item $16~\mathrm{Byte}$ per event per workspace for double-precision time-of-flight and pulse-time.
\end{itemize}
This is a naive model that assumes that we require a histogram representation of the data at some point during the reduction.
Histogram mode is used for visualization as well as data reduction steps that cannot be done in event mode, such as normalization.
Accounting for memory for a full histogram representation can lead to very high memory requirements, see in particular the discussion on \dream below.

\begin{table}
  \scriptsize
  \input{BEER.tex}
  \caption{\label{tab:hardware-beer}
Example estimate of hardware required for script-based reduction for \beer.
In this and the following tables, the `Pixels' column refers to the build-out phase.
`Use' is a guess of the fraction a particular instrument mode/configuration is used.
The values are chosen to add up to 100\% but could be adjusted to take into account time without beam and time for changing samples.
If the obtainable $\Treduction$ violates the conditions such as a $5\times$ speedup stated in Sec.~\ref{sec:scaling-analysis} the corresponding $\Treduction$ in the table is highlighted in \textcolor{red}{red}.
Note that in some cases $\Treduction$ is actually less than $30~\mathrm{s}$.
The reason is that the scripts generating these tables do not test all possible values of $\Ncore$, leading to jumps in $\Treduction$.
\\
For \beer the biggest uncertainty comes from the duration of a single run.
The measurement time for each individual measurement point in the sample can be very low.
If each such point is considered as an independent measurement we obtain a high hardware cost, especially in \emph{high-flux-multiplexing} mode.
If a combined treatment of the points in the sample scan is possible the requirement is likely to be much lower in the high-flux case.
For the \emph{high-flux-multiplexing} mode $\Trun$ looks unrealistic (below the pulse length), yielding an enormous core count.
The rate given by the instrument team for this mode is two orders of magnitude above the stated average rate so our naive computation of $\Trun$ may not correspond to the actual use case, i.e., the result should be interpreted with care.
For the \emph{medium-flux-multiplexing} mode we do not have an actual statement of the event rate but we have chosen to include it as an estimate for an intermediate case.
}
\end{table}

\begin{table}
  \footnotesize
  \input{BIFROST.tex}
  \caption{\label{tab:hardware-bifrost}
Example estimate of hardware required for script-based reduction for \bifrost.
Our information on \bifrost is a bit dated (from May 2017) so this table may be inaccurate since we were less aware of some factors that strongly influence hardware requirements.
The pixel count for \bifrost is very low, yielding a comparatively low hardware requirement.
However, the detector bank is scanned and data from multiple positions is to be included in the same reduction and analysis.
This is likely connected with an overhead which has not been included here.
}
\end{table}

\begin{table}
  \footnotesize
  \input{CSPEC.tex}
  \caption{\label{tab:hardware-cspec}
Example estimate of hardware required for script-based reduction for \cspec.
We assume that combining data from multiple runs (rotations) is \emph{not} part of data reduction and is handled by the analysis software.
If the transformation to $(\vec Q, \Delta E)$ and subsequent summation were to be done in \mantid the hardware requirement would increase significantly.
We currently assume that this would be done as part of the PACE project.
}
\end{table}

\begin{table}
  \footnotesize
  \input{DREAM.tex}
  \caption{\label{tab:hardware-dream}
Example estimate of hardware required for script-based reduction for \dream.
See the discussion in the text regarding the extreme memory requirement in \emph{high-resolution} mode.
$\Ncore$ is high and is dominated by the $\Nbin$ term of Eq.~\eqref{eq:master}.
Therefore $\Ncore$ also scales badly with $\Trun$.
Our estimate of $\Trun$ is currently quite uncertain and this table needs to be updated once we have access to a better estimate.
It should also be noted that while our benchmarks include the event-mode powder-diffraction workflow which will also be used for \dream, the benchmarks are based on POWGEN with only 43121 pixels, i.e., we are dealing with an extrapolation over more than two orders of magnitude.
}
\end{table}

\begin{table}
  \scriptsize
  \input{ESTIA.tex}
  \caption{\label{tab:hardware-estia}
Example estimate of hardware required for script-based reduction for \estia.
The average required core count is low.
With exception of the reference measurements in \emph{high-flux} mode the required hardware the peak core count is also low.
The peak core count can be reduced by accepting a longer reduction time for the reference measurements, which are infrequent and only of short duration.
}
\end{table}

\begin{table}
  \scriptsize
  \input{LoKI.tex}
  \caption{\label{tab:hardware-loki}
Example estimate of hardware required for script-based reduction for \loki.
Peak as well as average core count requirements are low.
}
\end{table}

\begin{table}
  \footnotesize
  \input{MAGIC.tex}
  \caption{\label{tab:hardware-magic}
Example estimate of hardware required for script-based reduction for \magic.
The numbers given here are very high --- and most likely unrealistic --- due to a combination of a high pixel count and a low event count per sample rotation angle, with only $10^5$ to $5\times10^5$ events per rotation angle quoted by the instrument scientists.
Assuming that each rotation is treated independently results in the figures given here.
Probably this is not a good assumption and a joint treatment of sample rotation angles needs to be considered instead.
}
\end{table}

Provided that the compute cluster is large enough, summing all $\Ncoremean$ for all relevant instrument configurations from Tabs.~\ref{tab:hardware-beer}--\ref{tab:hardware-magic} could yield the required size of the compute cluster.
However, there are clearly cases with high peak-core requirements.
Furthermore, in the early days with few operating instruments and a smaller cluster it needs to be ensured that an appropriate partition is always available within a to be defined time frame.
Determining the resulting required size of the compute cluster is beyond the scope of this investigation.

Instrument with a very high time resolution and high pixel count such as \dream suffer from an exceedingly high memory requirement.
In reality there are probably ways to alleviate this issue, arising from a full histogram representation of the data.
Working in event mode we have the following options:
\begin{enumerate}
  \item For visualization purposes, on-the-fly binning of event data could be implemented.
  \item The transition to histogram mode can be done histogram-by-histogram at the point of histogram summation, e.g., for powder diffraction workflows used for \dream in the algorithm \verb|DiffractionFoccusing|.
    In the case of powder diffraction this is already supported by the existing workflow.
    However, this is only possible provided that there are no issues arising from performing normalization in event mode, as described in \url{http://journals.iucr.org/j/issues/2016/02/00/fs5119/index.html}.
\end{enumerate}

One of the parameters that is hardest to pin down in discussions with the instrument teams is $\Trun$, which influences the hardware requirements in two ways:
\begin{itemize}
  \item Shorter $\Trun$ implies \emph{more} runs, i.e., more reductions.
  \item The $t_0$ term and the $\Nbin$ term in Eq.~\eqref{eq:master} become more dominant in relation to $\Nevent$ since the former two terms stay constant while the latter scales with $1/\Trun$.
    To fulfill the speedup conditions $\Ncore$ needs to be increased when $\Trun$ increases.
\end{itemize}
In combination this implies that in practice $\Ncoremean$ shows \emph{worse than linear} scaling with $1/\Trun$.\footnote{Unless we fix $\Ncore$ and drop our speedup conditions.}
Any uncertainty related to $\Trun$ will thus be amplified.




\section{Hardware components}


\subsection{Main memory}

Distinguishing between \emph{default} and \emph{high-mem} machine configurations probably makes sense.
With the parameter settings used for Tabs.~\ref{tab:hardware-beer}--\ref{tab:hardware-magic} we obtain a typical memory requirement of $\Mcore$ up to $\Mcore^{\text{default}} = 10~\mathrm{GByte}$.
\bifrost is likely to fall in the $\Mcore^{\text{default}}$ class even when considering a detector scan.
The are some major exceptions which we label $\Mcore^{\text{high-mem}}$ like \cspec with $\Mcore$ up to $30~\mathrm{GByte}$ and \dream exceeding $100~\mathrm{GByte}$ per core in its highest-resolution mode.
\magic is also in the $\Mcore^{\text{high-mem}}$ class, but the precise requirement depends on how the combination of rotation angles is handled.\footnote{The figures in Tab.~\ref{tab:hardware-magic} do \emph{not} explicitly include handling of multiple angles or merging, unless it is covered by the generic factor $\Nworkspace$.}

It is worth noting that a large part of the huge memory requirements for \dream and \magic come from the large number of detector pixels resulting in a large $\Nspec$.
Both instruments use the same volumetric detector technology with 32 layers of voxels.
This is similar for \cspec.
While there are long-term plans to use the depth information for ray-tracing, it is not absolutely necessary from the beginning.
One could argue that we can thus project the voxel data onto a grid of pixels, gaining an order of magnitude reduction of $\Nspec$.
However, we cannot rely on that at this point since there are a couple of complications:
\begin{enumerate}
  \item Voxels are not aligned with the secondary flight paths so any projection will need to rebin the data, splitting events into fractional events, unless done in histogram-mode.
  \item Voxel-layer-dependent calibration may be necessary, preventing and early transition to a lower $\Nspec$.
\end{enumerate}

We currently do not have benchmarks relating the \mantid performance to RAM bandwidth.
However, given the computational structure of \mantid, we expect that a higher RAM bandwidth is more important than a large number of cores per node.
We show an attempt to prove this in Fig.~\ref{fig:memory-bandwidth}.
However, since the effects of changing the number of used memory slots in a multi-core system are complex we should be careful to read to much into these results.

\begin{figure}
  \centering
\includegraphics[height=0.33\textheight]{../benchmarks/memory-bandwidth/memory-bandwidth.pdf}
\caption{\label{fig:memory-bandwidth}
Left panel:
Memory bandwidth as measured by \emph{pmbw} (\url{https://github.com/bingmann/pmbw}) depending on the number of equipped memory slots in a dual-socket Xeon E5-2620 v3 running at 2.40GHz.
The DIMMs are DDR4 running at 1866 MHz.
Right panel:
Runtime of a \mantid reduction with approximately $6\cdot10^8$ events.
With only a single DIMM the scaling with the number of MPI appears to be more limited.
}
\end{figure}


\subsection{Network}

For data reduction, \mantid does not put particularly high demands on the interconnect between nodes.
The most important factor is the speed of the connection to the file system.
During file system reads and writes there will also be significant communication between nodes.
Non-I/O algorithms in \mantid typically have either no communication or communication with only moderate latency and bandwidth requirements.
While Ethernet is not sufficient, our current feeling is that any Infiniband network is, even if it is an older revision.


\subsection{Filesystem I/O}

The filesystem bandwidth is crucial.
For Tabs.~\ref{tab:hardware-beer}--\ref{tab:hardware-magic} we used $\Bmax = 10^8~\mathrm{s}^{-1}$ to model the maximum read bandwidth \emph{for a particular reduction}, i.e., we assumed that the would be \emph{no resource contention}.
The quoted $\Bmax$ corresponds to an approximate bandwidth of $1~\mathrm{GByte/s}$, depending also on the \nexus compression level.
Determining when conflicts could arise is beyond the scope of this report.
If conflicts are likely $\Bmax$ should be reduced to incorporate the contention into the model.

While all workflows need to read significant amounts of data, the same is not true for writing:
\begin{itemize}
  \item \bifrost, \cspec, and potentially \magic will write large files, whereas the output files for \beer, \dream, \estia, and \loki are small.
  \item For interactive reduction we need to take into account saving and loading of \mantid project files which often contain all workspaces and can thus become very large.
\end{itemize}


\subsection{SSDs}

Fast local SSDs could potentially be helpful for inelastic and single crystal, handling large multi-dimensional workspaces exceeding the available RAM.
\mantid supports a file-backed mode for this case.
This is something we considered as part of an investigation by Anton Piccardo-Selg, see \url{https://github.com/DMSC-Instrument-Data/documents/tree/master/investigations/MultiDimensionalInvestigation}, but due to the complexity and uncertainty of how useful it would be in practice we did not include it in any of the considerations in this documents.
Nevertheless we should keep it in mind to widen our options in the future.




\section{Discussion}

(discussion will be added after initial review rounds)

We give an incomplete list of ``action items'' resulting from this report.
With a couple of exceptions, most of the items in this list are not single action items but will instead involve a long-running process and effort:
\begin{itemize}
  \item Progressively update parameter estimates.
  \item Ensure that estimates for \odin are included either here or in a similar report for data analysis.
  \item Obtain updated specifications for \bifrost.
  \item Develop better understanding of how we need to handle data from measurements with parameters scans.
    This can be sample rotation scans for single crystal diffraction and inelastic scattering, scans of the gauge volume for engineering diffraction, and sample environment parameter scans such as temperature scans.
    There is a variety of options to deal with this, such as handling everything independently, filtering during event loading, and filtering in memory.
    Since there is no single solution that works for all cases it is difficult to provide a generic estimate without going into the details of every workflow.
\end{itemize}




\appendix




\section{Related documentation}
\label{app:refs}

\begin{itemize}
  \item Repository containing scripts to produce data in this document as well as the source for this document:\\
    \url{https://github.com/DMSC-Instrument-Data/data-reduction-hardware-requirements}
  \item Initial thoughts and plan for estimating hardware requirements:\\
    \url{https://github.com/DMSC-Instrument-Data/documents/blob/master/mantid/hardware-requirements/plan.md}
  \item Notes from meetings with instrument teams:\\
    \url{https://confluence.esss.lu.se/display/DAM/Data+Reduction}
  \item Benchmarks for live reduction (Lamar Moore):\\
    \url{https://github.com/DMSC-Instrument-Data/documents/blob/master/investigations/Live%20Reduction/LiveReductionInvestigation.md}
  \item Benchmarks for event filtering (Neil Vaytet):\\
    \url{https://github.com/DMSC-Instrument-Data/data-reduction-hardware-requirements/blob/master/benchmarks/nvaytet/summary.md}
  \item \odin data rate report:\\
    \url{https://confluence.esss.lu.se/download/attachments/274116159/report_ODIN_data_rate_volume.pdf?api=v2}
\end{itemize}




\section{Motivation of the performance master equation}
\label{app:master_eq}

The rationale for Eq.~\eqref{eq:master} is as follows:

\begin{itemize}
  \item For the vast majority of algorithms used in data reduction, all spectra are treated independently.
  Thus there is a linear term in $\Nspec$ or $\Nbin$, but no higher order terms, and there is perfect scaling with $\Ncore$.
\item Events in \mantid are stored with their respective spectrum.
  Strictly speaking, we should thus include a term
  \begin{align}
    \Treduction^{\text{nonlinear}} &= \sum_{i=1}^{\Nspec} (N_{event,i} t_{event,linear} + N_{event,i} log N_{event,i} \Tevent^{\text{NlogN}} + ...),
    \intertext{
      i.e., a different term for each spectrum, depending on the number of events in that spectrum, and non-linear term, such as for sorting events.
    However, events are usually spread over many spectra, so we can approximate}
    N_{event,i} &\approx \Nevent/\Nspec.
    \intertext{We obtain}
    \Treduction^{\text{nonlinear}} &= \sum_{i=1}^{\Nspec} (\Nevent^i \Tevent^{\text{linear}} + \Nevent^i \log \Nevent^i \Tevent^{\text{NlogN}} + ...)\\
    &\approx \sum_{i=1}^{\Nspec} (\frac{\Nevent}{\Nspec}  \Tevent^{\text{linear}} + \Nevent/\Nspec \log \frac{\Nevent}{\Nspec}  \Tevent^{\text{NlogN}} + ...)\\
    &= \Nevent\Tevent^{\text{linear}} + \frac{\Nevent}{\Nspec} \sum_{i=1}^{\Nspec} (\log \frac{\Nevent}{\Nspec}  \Tevent^{\text{NlogN}} + ...)\\
    &= \Nevent\Tevent^{\text{linear}} + \Nevent (\log \frac{\Nevent}{\Nspec}  \Tevent^{\text{NlogN}} + ...).
    \intertext{The term in parenthesis depends on $\log (\Nevent/\Nspec)$ and \emph{not} $\log \Nevent$ (similar for higher order terms).
  These terms are thus typically small and it seems reasonable to absorb them into the linear term}
    &\approx \Nevent*\Tevent.
  \end{align}
  If instead events are peaked in a subset of spectra we approximate $N_{event,i} ~ (0 \text{ or } \Nevent/N_{\text{peak}}$, the approximation works in a similar way.
\item Loading large event files is a significant contribution to the overall reduction time.
  While the exact scaling of the new parallel event loader is unknown (no adequate parallel file system available), there is definitely a linearly scaling contribution that scales well with the number of cores and is thus already described by the $\Nevent*\Tevent$ term.
  In addition to that, the other major factor is given by the upper limit of file system bandwidth.
  Basically, this is a limit to the number of events that can be loaded per second.
  This will also depend on whether or not compression is used in NeXus files.
  We model this limit in the equation with the term $\Nevent/\Bmax$.
  In case a parallel file system provides an bandwidth that is much higher on average than what was benchmarked for a local SSD, we may need to include a different term that captures limited scaling of the parallel loader.
\item The time for reducing a spectrum will often depend linearly on the bin count.
  Many instrument can adjust their resolution, usually by sacrificing brightness.
  Thus the bin count will be fixed for a given run but can vary between runs within a instrument-specific range.
\end{itemize}




\section{Instrument parameters}
\label{app:instrument-parameters}


\subsection{\beer}

\begin{itemize}
  \item According to the original proposal, resolution settings range from 0.1\% to 1\%.
    The wavelength band is $1.7~\angstrom$ but there are chopper settings to double it using pulse skipping.
    For a wavelength band centered at $2~\angstrom$ this yields bins counts of $10\times1.7~\angstrom/(0.01\cdot2~\angstrom) = 850$ for the low resolution setting and $10\times1.7~\angstrom/(0.001\cdot2~\angstrom) = 8500$ for the highest resolution.
\end{itemize}


\subsection{\bifrost}

\begin{itemize}
  \item According to the original proposal, resolution settings range from 1\% to 4\%.
  \item The wavelength band is $1.7~\angstrom$.
    In the most extreme case of 1\% resolution for a central wavelength of $2~\angstrom$ this yields approximately $10\times1.7~\angstrom/(0.01\cdot2.0~\angstrom) = 850$.
    At $4~\angstrom$ with 4\% resolution we obtain $10\times1.7~\angstrom/(0.02\cdot4.0~\angstrom) = 110$.
\end{itemize}


\subsection{\dream}

\begin{itemize}
  \item The (useful) range of the pulse shaping is $10~\mathrm{\mu s}$ to $500~\mathrm{\mu s}$.
    We thus obtain $10\times71~\mathrm{ms}/10~\mathrm{\mu s} = 71000$ bins for the highest resolution and $10\times71~\mathrm{ms}/500~\mathrm{\mu s} = 1420$ for the lowest resolution.
\end{itemize}


\subsection{\estia}

From the original proposal:

\begin{itemize}
  \item Wavelength range is $5~\angstrom$ to $9.4~\angstrom$ with an intrinsic resolution of 2\% and 4\%, respectively.
    In the simplest case with constant bin width this yields $10\times(9.4-5.0)/(0.02\cdot5.0) = 450$ bins, where the factor of 10 is the aforementioned rule of thumb to obtain a required bin count from a resolution.
\end{itemize}


\subsection{\loki}

\begin{itemize}
  \item Resolution is dominated by the pulse length.
    We thus obtain an approximate bin count of  $10\times71~\mathrm{ms}/3~\mathrm{ms} = 240$.
\end{itemize}


\subsection{\magic}

\begin{itemize}
  \item The shortest pulse from the pulse-shaping chopper is $100~\mathrm{\mu s}$ yielding $10\times71~\mathrm{ms}/100~\mathrm{\mu s} = 7100$ bins, where $71~\mathrm{ms}$ is the time to the next pulse.
    While there is a high-flux option, my understanding is that this depends on collimation while the pulse-shaping is unchanged, i.e., we always have this pulse length.
\end{itemize}




\section{Helper scripts}
\label{app:scripts}

Use \verb|generate_tables.py| to generate a table output on the command line as well as input files for this LaTex document.
High-level parameters are currently set in \verb|performance_model.py| and \verb|beamline.py|.
Instrument-specific settings can be found in \verb|generate_tables.py|.

\end{document}
